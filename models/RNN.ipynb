{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6989538,"sourceType":"datasetVersion","datasetId":4017254},{"sourceId":7245991,"sourceType":"datasetVersion","datasetId":4197522},{"sourceId":7246133,"sourceType":"datasetVersion","datasetId":4197628}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt install default-jre","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get --yes --force-yes install default-jdk ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import one_hot\nimport torch.optim as optim \nfrom torch.optim import lr_scheduler\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter \nfrom gensim.utils import simple_preprocess \nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN = True #from scratch\nPRETRAINED = False\nINFER = True\nTEST = True\nassert TRAIN != PRETRAINED, 'TRAIN and PRETRAINED should be different'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 1 \nlearning_rate = 1e-04\ndisplay_step = 2500\nepochs = 25\nnum_classes = 7\nmax_len = 75\npretrained_segmentation = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_path = '/kaggle/input/pretrained-rnn/RNN.pth'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install py_vncorenlp\nimport py_vncorenlp\nimport os\nif not os.path.exists('/kaggle/working/vncorenlp'):\n    os.makedirs('/kaggle/working/vncorenlp')\n    \npy_vncorenlp.download_model(save_dir='/kaggle/working/vncorenlp'); \nrdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='/kaggle/working/vncorenlp');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data(path):\n    data = pd.read_excel(path, sheet_name=None)['Sheet1']\n    data.columns = ['index', 'Emotion', 'Sentence'] \n    data.drop(columns=['index'], inplace=True)\n    return data\n\ntrain_data = get_data('/kaggle/input/uit-vsmec/UIT-VSMEC/train_nor_811.xlsx')\nvalid_data = get_data('/kaggle/input/uit-vsmec/UIT-VSMEC/valid_nor_811.xlsx')\ntest_data = get_data('/kaggle/input/uit-vsmec/UIT-VSMEC/test_nor_811.xlsx') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2vec_path = '/kaggle/input/word2vec-300dim/word2vec_vi_words_300dims.txt'\nfile = open(word2vec_path, \"r\")\nnlines, ebb_dim = file.readline().split() \nnlines, ebb_dim = int(nlines), int(ebb_dim)\nword2vec_idx = dict()\nword2vec_weights = list()\nword2vec_weights.append([0 for j in range(ebb_dim)])\nprint('getting embeddings:')\nfor i in tqdm(range(1, nlines + 1)):\n    line = file.readline() \n    while line != \"\":  \n        line = line.split()\n        word = \"_\".join(line[0:len(line)-ebb_dim]).lower()\n        if word not in word2vec_idx:\n            word2vec_weights.append([float(j) for j in line[len(line)-ebb_dim:len(line)]]) \n            word2vec_idx[word] = i  \n            break  \n        line = file.readline()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_map = {\n    \"òa\": \"oà\",\n    \"Òa\": \"Oà\",\n    \"ÒA\": \"OÀ\",\n    \"óa\": \"oá\",\n    \"Óa\": \"Oá\",\n    \"ÓA\": \"OÁ\",\n    \"ỏa\": \"oả\",\n    \"Ỏa\": \"Oả\",\n    \"ỎA\": \"OẢ\",\n    \"õa\": \"oã\",\n    \"Õa\": \"Oã\",\n    \"ÕA\": \"OÃ\",\n    \"ọa\": \"oạ\",\n    \"Ọa\": \"Oạ\",\n    \"ỌA\": \"OẠ\",\n    \"òe\": \"oè\",\n    \"Òe\": \"Oè\",\n    \"ÒE\": \"OÈ\",\n    \"óe\": \"oé\",\n    \"Óe\": \"Oé\",\n    \"ÓE\": \"OÉ\",\n    \"ỏe\": \"oẻ\",\n    \"Ỏe\": \"Oẻ\",\n    \"ỎE\": \"OẺ\",\n    \"õe\": \"oẽ\",\n    \"Õe\": \"Oẽ\",\n    \"ÕE\": \"OẼ\",\n    \"ọe\": \"oẹ\",\n    \"Ọe\": \"Oẹ\",\n    \"ỌE\": \"OẸ\",\n    \"ùy\": \"uỳ\",\n    \"Ùy\": \"Uỳ\",\n    \"ÙY\": \"UỲ\",\n    \"úy\": \"uý\",\n    \"Úy\": \"Uý\",\n    \"ÚY\": \"UÝ\",\n    \"ủy\": \"uỷ\",\n    \"Ủy\": \"Uỷ\",\n    \"ỦY\": \"UỶ\",\n    \"ũy\": \"uỹ\",\n    \"Ũy\": \"Uỹ\",\n    \"ŨY\": \"UỸ\",\n    \"ụy\": \"uỵ\",\n    \"Ụy\": \"Uỵ\",\n    \"ỤY\": \"UỴ\",\n    }\n\ndef tone_normalize(text, dict_map):\n    for i, j in dict_map.items():\n        text = text.replace(i, j)\n    return text\n\ndef preprocess_text(text):\n    text = \" \".join(simple_preprocess(text)).strip()\n    \n    output = rdrsegmenter.word_segment(text)\n    text = \" \".join(output)     \n    \n    text = tone_normalize(text, dict_map)  \n    return text\n\ndef clean_text(text): \n    text = preprocess_text(text) \n    tokens = text.split()  \n    tokens = [w for w in tokens if w in word2vec_idx]\n    if (max_len != -1):\n        tokens = tokens[0:max_len]     \n    tokens = ' '.join(tokens).strip()\n    return tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def emotions_map(text):\n        if text=='Enjoyment':\n            return 0\n        elif text=='Disgust':\n            return 1\n        elif text=='Sadness':\n            return 2\n        elif text=='Anger':\n            return 3\n        elif text=='Surprise':\n            return 4\n        elif text=='Fear':\n            return 5\n        else:\n            return 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['Sentence'] = train_data['Sentence'].map(clean_text)\nvalid_data['Sentence'] = valid_data['Sentence'].map(clean_text)\n\ntrain_data['Emotion'] = train_data['Emotion'].map(emotions_map)\nvalid_data['Emotion'] = valid_data['Emotion'].map(emotions_map) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts = [1/(train_data['Emotion'].value_counts()[i]) for i in range(num_classes)]\nweights = [i/(sum(counts)) for i in counts]\nweights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataClass(Dataset):\n    def __init__(self, input_list, output_list, max_len = max_len, train=False): \n        super(DataClass, self).__init__()\n        self.input_list = input_list\n        self.output_list = output_list\n        self.train = train\n        self.max_len = max_len\n \n        encoded_text = [torch.Tensor([word2vec_idx[word] for word in sentence.strip().split() if word in word2vec_idx] ).to(device) for sentence in self.input_list]  \n        self.input_list = encoded_text \n        self.output_list = [torch.tensor(int(y)) for y in self.output_list]\n        self.output_list = [one_hot(y, num_classes = num_classes) for y in self.output_list]\n        \n    def __getitem__(self, index):   \n        data = self.input_list[index]\n        label = self.output_list[index]\n       \n        return data, label\n    \n    def __len__(self):\n        return len(self.input_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input_list = [x for x in train_data[\"Sentence\"]]\ntrain_output_list = [x for x in train_data[\"Emotion\"]]\ntrain = [x for x in list(zip(train_input_list, train_output_list)) if len(x[0]) >0] \ntrain_input_list, train_output_list  = [list(t) for t in zip(*train)]\n\nvalid_input_list = [x for x in valid_data[\"Sentence\"]]\nvalid_output_list = [x for x in valid_data[\"Emotion\"]]\nvalid = [x for x in list(zip(valid_input_list, valid_output_list)) if len(x[0]) >0] \nvalid_input_list, valid_output_list  = [list(t) for t in zip(*valid)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = DataClass(train_input_list, train_output_list)\nvalid_set = DataClass(valid_input_list, valid_output_list)\n\ntrain_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nvalid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model(model, optimizer, path):\n    checkpoint = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, path)\n    print('model saved')\n\ndef load_model(model, optimizer, path):\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint[\"model\"])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    return model, optimizer\n    print(\"model loaded\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Rnn(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Rnn, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(word2vec_weights).to(device))\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        x = x.long() \n        x = self.embedding(x)  \n        out, _ = self.rnn(x) \n        out = self.fc(out[:, -1, :])\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import OrderedDict\n\nmodel = Rnn(ebb_dim, 50, 7) \n\nif PRETRAINED == True:\n    checkpoint = torch.load(pretrained_path) \n\n    new_state_dict = OrderedDict()\n    for k, v in checkpoint['model'].items(): \n        name = k\n        new_state_dict[name] = v \n        \n    model.load_state_dict(new_state_dict)  \n    \nmodel.to(device)\nsummary(model, torch.Size([max_len]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_dataloader, \n          valid_dataloader,\n          epoch, display_step):\n    \n    start_time = time.time()\n    train_loss_epoch = 0\n    valid_loss_epoch = 0\n    train_correct = 0\n    valid_correct = 0\n    last_loss = 999999999  \n\n    model.train()\n    for i, (data,targets) in enumerate(train_dataloader):\n        \n        data, targets = data.to(device), targets.to(device) \n         \n        optimizer.zero_grad()\n        \n        outputs = model(data) \n        \n        loss = loss_function(outputs, torch.max(targets, dim = 1)[1])\n        loss.backward()\n        \n        optimizer.step()\n        \n        train_correct += (torch.max(outputs, dim = 1)[1] == torch.max(targets, dim = 1)[1]).sum().float()\n\n        train_loss_epoch += loss.item()  \n        \n        if (i+1) % display_step == 0:\n            print('Train Epoch: {} [{}/{} ({}%)] '.format(\n                epoch + 1, (i+1) * len(data), len(train_dataloader.dataset), 100 * (i+1) * len(data) / len(train_dataloader.dataset)  \n            ))     \n                            \n    train_loss_epoch/= len(train_set) \n    train_accuracy = train_correct / len(train_set) \n \n    model.eval()\n    with torch.no_grad():\n        for data, target in valid_dataloader:\n            data, target = data.to(device), target.to(device)\n            valid_output = model(data)\n            valid_loss = nn.CrossEntropyLoss()(valid_output, target.float())    \n            valid_loss_epoch += valid_loss.item()\n            valid_correct += (torch.max(valid_output, dim = 1)[1] == torch.max(target, dim = 1)[1]).sum().float() \n              \n    valid_loss_epoch/= len(valid_set)\n    valid_accuracy = valid_correct / len(valid_set) \n        \n\n    print(f\"Done epoch #{epoch+1}, time for this epoch: {time.time()-start_time}s\") \n    \n    train_accuracy = train_accuracy.item()\n    valid_accuracy = valid_accuracy.item()\n    \n    return train_loss_epoch , valid_loss_epoch, train_accuracy, valid_accuracy    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_function = nn.CrossEntropyLoss(torch.Tensor(weights).to(device)) \noptimizer = optim.AdamW(params=model.parameters(), lr=learning_rate )\n\nif PRETRAINED == True:\n    optimizer.load_state_dict(checkpoint['optimizer'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN == True:\n    for epoch in range(epochs):\n        train_loss_epoch = 0\n        test_loss_epoch = 0 \n\n        (train_loss_epoch, valid_loss_epoch, train_accuracy, valid_accuracy ) = train(train_dataloader, valid_dataloader, epoch, display_step)\n        print('Epoch: ' + str(epoch + 1) + '\\tTrain_loss: '+str(train_loss_epoch)+ '\\tTrain_acc: ' + str(train_accuracy) + '\\tVal_loss: ' + str(valid_loss_epoch) + '\\tVal_acc: ' + str(valid_accuracy) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['Sentence'] = test_data['Sentence'].map(clean_text) \ntest_data['Emotion'] = test_data['Emotion'].map(emotions_map)\n\ntest_output_list = [x for x in test_data[\"Emotion\"]]\ntest_input_list = [x for x in test_data[\"Sentence\"]]\ntest = [x for x in list(zip(test_input_list, test_output_list)) if len(x[0]) >0] \ntest_input_list, test_output_list  = [list(t) for t in zip(*test)]\n\ntest_set = DataClass(test_input_list, test_output_list) \ntest_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TEST == True:\n    model.eval() \n    predicts = list()\n    outputs = list()\n    with torch.no_grad():\n        for data, target in test_dataloader:\n            data, target = data.to(device), target.to(device)\n            test_output = model(data) \n            predicts.append(torch.max(test_output, dim = 1)[1].item())\n            outputs.append(torch.max(target, dim = 1)[1].item())  \n \n    print()\n    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n    print(classification_report(predicts, outputs, digits = 4)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from string import punctuation \ndef reverse_emotion_map(emotion): \n        if emotion == 0:\n            return 'Enjoyment'\n        elif emotion == 1:\n            return 'Disgust'\n        elif emotion == 2:\n            return 'Sadness'\n        elif emotion == 3:\n            return 'Anger'\n        elif emotion == 4:\n            return 'Surprise'\n        elif emotion == 5: \n            return \"Fear\"\n        else:\n            return \"Other\"\n        \ndef infer(string):\n    model.eval() \n    for punct in punctuation: \n        string = string.replace(punct, \" \" + punct + \" \") \n    old_str = \" \"\n    while old_str != string:\n        old_str = string\n        string.replace(\"  \", \" \")\n    string = string.lower() \n    string = clean_text(string) \n    encoded = torch.Tensor([word2vec_idx[word] for word in string.strip().split() if word in word2vec_idx]).to(device)\n    encoded = torch.unsqueeze(encoded,0)\n    emotion = model(encoded)  \n    emotion = torch.max(emotion, dim = 1)[1].item()           \n    emotion = reverse_emotion_map(emotion)\n    return emotion           ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_str = \"Hê\" \nprint(infer(infer_str))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
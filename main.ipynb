{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1719b872",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/minhvn1433/Deep-learning-project/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2744c5",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "This is a testing notebook. In this notebook, you will play with a csv file. ðŸ˜ŽðŸ˜ŽðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43dd72",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "You will first import common libraries that will be used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Embedding,\n",
    "    Conv1D,\n",
    "    Bidirectional,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b481796",
   "metadata": {},
   "source": [
    "### Load and Prepare the Dataset\n",
    "\n",
    "First, you will load the csv file and extract the contents into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d544991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('data.csv')\n",
    "display(df)\n",
    "\n",
    "# Initialize the lists\n",
    "sentences = df['comment'].tolist()\n",
    "labels = df['rate'].tolist()\n",
    "labels = [label - 1 for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f369b",
   "metadata": {},
   "source": [
    "You will then split the lists into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa4a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train data\n",
    "(\n",
    "    training_sentences,\n",
    "    temp_sentences,\n",
    "    training_labels,\n",
    "    temp_labels\n",
    ") = train_test_split(sentences, labels, test_size=0.2, stratify=labels)\n",
    "\n",
    "# Split the validation and test data\n",
    "(\n",
    "    validation_sentences,\n",
    "    testing_sentences,\n",
    "    validation_labels,\n",
    "    testing_labels,\n",
    ") = train_test_split(temp_sentences, temp_labels, test_size=0.5, stratify=temp_labels)\n",
    "\n",
    "# Convert the labels lists into numpy arrays\n",
    "training_labels = np.array(training_labels)\n",
    "validation_labels = np.array(validation_labels)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee3721",
   "metadata": {},
   "source": [
    "Next, you will generate the vocabulary and padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44faabe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 300\n",
    "max_length = 304\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Generate and pad the training sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the validation sequences\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the testing sequences\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcb967",
   "metadata": {},
   "source": [
    "### Plot Ultility\n",
    "\n",
    "Before you define the models, you will define the function below so you can easily visualize the accuracy and loss history after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Ultility\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f183c92",
   "metadata": {},
   "source": [
    "### Model 1: Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da45952",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "# Build the models\n",
    "inputs = Input(shape=(max_length,), dtype='int32')\n",
    "X = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "X = Flatten()(X)\n",
    "X = Dense(16, activation='relu')(X)\n",
    "X = Dense(5, activation='softmax')(X)\n",
    "model = Model(inputs=inputs, outputs=X)\n",
    "\n",
    "# Compile the model and print the model summary\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27708d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    training_padded,\n",
    "    training_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(validation_padded, validation_labels),\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ed9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss history\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f1155",
   "metadata": {},
   "source": [
    "### Model 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceded759",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "# Build the models\n",
    "inputs = Input(shape=(max_length,), dtype='int32')\n",
    "X = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "X = Bidirectional(LSTM(32))(X)\n",
    "X = Dense(16, activation='relu')(X)\n",
    "X = Dense(5, activation='softmax')(X)\n",
    "model_lstm = Model(inputs=inputs, outputs=X)\n",
    "\n",
    "# Compile the model and print the model summary\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Train the model\n",
    "history_lstm = model_lstm.fit(\n",
    "    training_padded,\n",
    "    training_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(validation_padded, validation_labels),\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss history\n",
    "plot_graphs(history_lstm, 'accuracy')\n",
    "plot_graphs(history_lstm, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e5880f",
   "metadata": {},
   "source": [
    "### Model 3: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "# Build the models\n",
    "inputs = Input(shape=(max_length,), dtype='int32')\n",
    "X = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "X = Bidirectional(GRU(32))(X)\n",
    "X = Dense(16, activation='relu')(X)\n",
    "X = Dense(5, activation='softmax')(X)\n",
    "model_gru = Model(inputs=inputs, outputs=X)\n",
    "\n",
    "# Compile the model and print the model summary\n",
    "model_gru.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6928a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Train the model\n",
    "history_gru = model_gru.fit(\n",
    "    training_padded,\n",
    "    training_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(validation_padded, validation_labels),\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da6a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss history\n",
    "plot_graphs(history_gru, 'accuracy')\n",
    "plot_graphs(history_gru, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01e1a0",
   "metadata": {},
   "source": [
    "### Model 4: Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "# Build the models\n",
    "inputs = Input(shape=(max_length,), dtype='int32')\n",
    "X = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "X = Conv1D(128, 5, activation='relu')(X)\n",
    "X = GlobalAveragePooling1D()(X)\n",
    "X = Dense(16, activation='relu')(X)\n",
    "X = Dense(5, activation='softmax')(X)\n",
    "model_conv = Model(inputs=inputs, outputs=X)\n",
    "\n",
    "# Compile the model and print the model summary\n",
    "model_conv.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Train the model\n",
    "history_conv = model_conv.fit(\n",
    "    training_padded,\n",
    "    training_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(validation_padded, validation_labels),\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss history\n",
    "plot_graphs(history_conv, 'accuracy')\n",
    "plot_graphs(history_conv, 'loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
